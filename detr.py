# -*- coding: utf-8 -*-
"""DETR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AyIf33iYobVapl6jQR9V6FMCbTMW2ORo
"""

!pip install -q transformers evaluate timm albumentations

from google.colab import drive
drive.mount('/content/drive')

"""### DATASET"""

from torch.utils.data import Dataset
import pandas as pd
import numpy as np
from PIL import Image, ImageDraw

class DetectionDataset(Dataset):
    def __init__(self, root, meta_path, transform, image_processor, split='train', train_val_ratio=0.7):
        super().__init__()
        self.root = root
        self.split = split
        self.transform = transform
        self.image_processor = image_processor

        if split in ['train', 'val']:
            self.load_meta(meta_path, train_val_ratio)
        else:
            self.load_test_meta(meta_path)

    def load_test_meta(self, meta_path):
        raw_meta = pd.read_csv(meta_path)
        self.meta = []
        n = raw_meta.shape[0]

        for i in range(n):
            image_id = raw_meta['image_id'][i]
            image_path = raw_meta['file_path'][i]
            self.meta.append({'image_id': image_id, 'image_path': image_path})

    def load_meta(self, meta_path, train_val_ratio):
        raw_meta = pd.read_csv(meta_path)
        self.meta = []
        n = raw_meta.shape[0]

        image = Image.open(self.root + raw_meta['file_path'][0][1:])
        width, height = image.size

        for i in range(n):
            image_id = raw_meta['image_id'][i]
            image_path = raw_meta['file_path'][i]
            category_id = raw_meta['category_id'][i]
            bbox = self.bbox_check((raw_meta['x'][i], raw_meta['y'][i], raw_meta['w'][i], raw_meta['h'][i]), width, height)
            area = bbox[2] * bbox[3]

            if image_id > len(self.meta):
                self.meta.append({
                    'image_id': image_id,
                    'image_path': image_path,
                    'objects': {
                        'category': [category_id],
                        'bbox': [bbox],
                        'area': [area],
                    }
                })
            else:
                self.meta[image_id - 1]['objects']['category'].append(category_id)
                self.meta[image_id - 1]['objects']['bbox'].append(bbox)
                self.meta[image_id - 1]['objects']['area'].append(area)

        n_samples = len(self.meta)
        n_train = int(n_samples * train_val_ratio)

        if self.split == 'train':
            self.meta = self.meta[:n_train]
        elif self.split == 'val':
            self.meta = self.meta[n_train:]

    def __getitem__(self, idx):
        if self.split == 'test':
            return self.get_testitem(idx)

        image = Image.open(self.root + self.meta[idx]['image_path'][1:])
        image = np.array(image.convert("RGB"))[:, :, ::-1]
        out = self.transform(
            image=image,
            bboxes=self.meta[idx]['objects']['bbox'],
            category=self.meta[idx]['objects']['category']
        )

        target = {
            "image_id": self.meta[idx]['image_id'],
            "annotations": self.formatted_anns(self.meta[idx]['image_id'],
                                               out['category'],
                                               self.meta[idx]['objects']['area'],
                                               out['bboxes']
                                              )
        }

        return self.image_processor(
            images=out['image'],
            annotations=target,
            return_tensors='pt'
        )

    def get_testitem(self, idx):
        return self.meta[idx]['image_id'], Image.open(self.root + self.meta[idx]['image_path'][1:])

    def bbox_check(self, bbox, width, height):
        x, y, w, h = bbox
        if x < 0.0:
            w -= x
            x = 0
        if x + w >= width:
            w -= (x + w) - width
        if y < 0.0:
            h -= y
            y = 0
        if y + h > height:
            h -= (y + h) - height
        return x, y, w, h

    def formatted_anns(self, image_id, category, area, bbox):
        annotations = []
        for i in range(len(category)):
            annotations.append({
                'image_id': image_id,
                'category_id': category[i],
                'isCrowd': 0,
                'area': area[i],
                'bbox': bbox[i],
            })
        return annotations

    def visualize(self, idx):
        img_path = self.meta[idx]['image_path']
        img = Image.open(self.root + img_path[1:])
        draw = ImageDraw.Draw(img)

        for x, y, w, h in self.meta[idx]['objects']['bbox']:
            draw.rectangle((x, y, x + w, y + h), outline='red', width=1)

        return img

    def __len__(self):
        return len(self.meta)

import albumentations
import numpy as np
import torch


transform = albumentations.Compose(
    [
        albumentations.Resize(480, 480),
        albumentations.HorizontalFlip(p=1.0),
        albumentations.RandomBrightnessContrast(p=1.0),
    ],
    bbox_params=albumentations.BboxParams(format="coco", label_fields=["category"]),
)

train_ds = DetectionDataset(
    '/content/drive/MyDrive/DL_Project/Dataset/SODA10M/train/',
    '/content/drive/MyDrive/DL_Project/Dataset/SODA10M/train/train_source.csv',
    transform,
)

val_ds = DetectionDataset(
    '/content/drive/MyDrive/DL_Project/Dataset/SODA10M/train/',
    '/content/drive/MyDrive/DL_Project/Dataset/SODA10M/train/train_source.csv',
    transform,
    split='val',
)

print('Load Dataset')

"""### VISUALIZATION"""

train_ds.visualize(300)

df = pd.read_csv('/content/drive/MyDrive/DL_Project/Dataset/SODA10M/category.csv')
n = df.shape[0]

id2label = {}
label2id = {}

for i in range(n):
    Id = int(df['category_id'][i])
    Name = df['name'][i]
    id2label[Id] = Name
    label2id[Name] = Id

print(id2label)
print(label2id)

"""### MODEL"""

!git clone https://github.com/facebookresearch/detr.git
!pip install -r detr/requirements.txt

import torch

# Load the pretrained DETR model with a ResNet-50 backbone
model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)

# If you need to fine-tune the model on your dataset, set it to training mode
model.train()

# Move the model to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

from transformers import AutoModelForObjectDetection

checkpoint = 'facebook/detr-resnet-50'

model = AutoModelForObjectDetection.from_pretrained(
    checkpoint,
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True,
)

"""### TRAIN"""

import torch
from torch.utils.data import DataLoader
from torch.optim import Adam
import torchmetrics
from torchvision.ops import box_convert

class Trainer:
    def __init__(self, train_dataset, val_dataset, model, training_args):
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.model = model
        self.training_args = training_args

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)

        self.train_loader = DataLoader(train_dataset, batch_size=training_args['batch_size'], shuffle=True)
        self.val_loader = DataLoader(val_dataset, batch_size=training_args['batch_size'], shuffle=False)

        self.optimizer = Adam(model.parameters(), lr=training_args['learning_rate'])

    def train_one_epoch(self):
        self.model.train()
        total_loss = 0
        for images, targets in self.train_loader:
            images = list(image.to(self.device) for image in images)
            targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]

            loss_dict = self.model(images, targets)
            losses = sum(loss for loss in loss_dict.values())

            self.optimizer.zero_grad()
            losses.backward()
            self.optimizer.step()

            total_loss += losses.item()
        return total_loss / len(self.train_loader)

    def train(self):
        best_val_loss = float('inf')
        for epoch in range(self.training_args['num_epochs']):
            train_loss = self.train_one_epoch()
            val_loss = self.validate()

            print(f"Epoch {epoch + 1}/{self.training_args['num_epochs']}, Train Loss: {train_loss}, Validation Loss: {val_loss}")

            # Save the model if it has improved
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(self.model.state_dict(), 'best_model.pth')

    def validate(self):
        self.model.eval()
        total_loss = 0
        metric = torchmetrics.detection.mean_ap.MeanAveragePrecision()

        with torch.no_grad():
            for images, targets in self.val_loader:
                images = list(image.to(self.device) for image in images)
                targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]

                outputs = self.model(images)

                for output, target in zip(outputs, targets):
                    # Convert boxes to expected format
                    pred_boxes = box_convert(output['boxes'], 'xyxy', 'xywh')
                    target_boxes = box_convert(target['boxes'], 'xyxy', 'xywh')

                    # Prepare the data for torchmetrics
                    res = {
                        'pred_boxes': pred_boxes.cpu(),
                        'pred_scores': output['scores'].cpu(),
                        'pred_labels': output['labels'].cpu(),
                        'target_boxes': target_boxes.cpu(),
                        'target_labels': target['labels'].cpu()
                    }

                    metric.update([res])

            # Compute the final mAP value
            mAP = metric.compute()
            print(f"Validation mAP: {mAP}")
            return total_loss / len(self.val_loader)

# Example of training_args
training_args = {
    'batch_size': 16,
    'learning_rate': 1e-4,
    'num_epochs': 2
}


# Create the Trainer instance
trainer = Trainer(train_ds, val_ds, model, training_args)
trainer.train()

from huggingface_hub import notebook_login

notebook_login()

from transformers import Trainer

def collate_fn(batch):
    pixel_values = [item["pixel_values"][0] for item in batch]
    encoding = image_processor.pad(pixel_values, return_tensors="pt")
    labels = [item["labels"][0] for item in batch]
    batch = {}
    batch["pixel_values"] = encoding["pixel_values"]
    batch["pixel_mask"] = encoding["pixel_mask"]
    batch["labels"] = labels
    return batch

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=collate_fn,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    tokenizer=image_processor,
)

trainer.train()

"""### VISUALIZATION"""

from huggingface_hub import notebook_login

notebook_login()

test_ds = DetectionDataset(
    '/content/drive/MyDrive/DL_Project/Dataset/SODA10M/test/',
    '/content/drive/MyDrive/DL_Project/Dataset/SODA10M/test/test.csv',
    None,
    None,
    split='test',
)

"""### SUBMIT"""